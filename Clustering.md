# Imitating Sentence Flow with Clustering

## Background

A couple of weeks ago, I had tried to cluster sentences in _Northanger Abbey_ using the occurrence of bigrams as features. I chose bigrams because, in the Markov chain experiments, more coherent sentences were generated when _n > 1_ versus when _n = 1_. I realized later that this reasoning didn't make sense. With the HMM, I wanted to infer topics in the prose and the flow from one topic to another -- a higher level process than sentence planning, which is where the conditional probabilities of individual words would matter.

I came up with a new idea, which was to use an HMM to model the flow of topics (which would be the hidden states), and have a different Markov chain for each state to actually generate the language. 

As before, I started by clustering sentences based on the presence or absence of certain words, but instead of bigrams, I just used unigrams for the features. The clusters formed were much more evenly distributed than those formed from bigram-based vectors.

## Clusters

Below are randomly chosen sentences per cluster, this time from _Persuasion_:

### Cluster 0
-   Anne Elliot, with all her claims of birth, beauty, and mind, to throw herself away at nineteen; involve herself at nineteen in an engagement with a young man, who had nothing but himself to recommend him, and no hopes of attaining affluence, but in the chances of a most uncertain profession, and no connexions to secure even his farther rise in the profession, would be, indeed, a throwing away, which she grieved to think of
-   She had imagined such difficulties of fortune to exist there as must prevent the marriage from being near at hand; but she learned from Charles that, very recently, (since Mary's last letter to herself), Charles Hayter had been applied to by a friend to hold a living for a youth who could not possibly claim it under many years; and that on the strength of his present income, with almost a certainty of something more permanent long before the term in question, the two families had consented to the young people's wishes, and that their marriage was likely to take place in a few months, quite as soon as Louisa's

### Cluster 1
- We asked him to come home with us for a day or two:  Charles undertook to give him some shooting, and he seemed quite delighted, and, for my part, I thought it was all settled; when behold 
- I cannot pretend to remember it, but it was something very fine--I overheard him telling Henrietta all about it; and then 'Miss Elliot' was spoken of in the highest terms

### Cluster 2
- ”  “You think it is all for ambition, then
-  I am so glad it is over
-  I have no notion of loving people by halves; it is not my nature

### Cluster 3
-   It would be hard, indeed" (with a faltering voice), "if woman's feelings were to be added to all this
- "  "That is precisely what I was going to observe," cried Mrs Croft
-   How was such jealousy to be quieted

### Cluster 4
-   You must not run away from us now
- ' 'Sixty,' said I, 'or perhaps sixty-two
- "  "Do you think so
-   She is fastidious
-   But, it had better not be attempted, perhaps

The clusters seem meaningful. Cluster 0 seems to encompass the narrator's voice. Cluster 1 consists of characters narrating things that have happened in the past. Clusters 2 through 4 consist of dialogue containing emotion or opinion. 

After clustering, I estimated state transition probabilities by using the clustering labels as state assignments. This was the next step in building the HMM, but it also turned out to yield pretty good results in terms of prose generation itself.

First, I generated state sequences using the state transition probabilities. Next, I created a Markovify (Markov chain) model for each state or topic, by training the model on all of the sentences in that topic/cluster. Then I iterated through the state sequence and had the appropriate Markov chain generate a sentence at each state.

## Generated paragraphs

The generated paragraphs, of course, are meaningless, but they do read significantly better (in my opinion, and also my friend's opinion) than the sentences that are generated by a single Markovify model trained on the entire text.

Below are some paragraphs generated using this approach:

- Believe it to be composed, and to command his prosperous path. Anne could not but yield to such directly opposite notions. It determined him to leave him yourself, but you will not be invited. But to be near enough to struggle with. She came away from Marlborough Buildings only on Sunday; and she could not but yield to such directly opposite notions. He saw her too; yet he looked grave, and seemed only to be called your own.
- How the long stage would pass; how it was to be introduced to each other's ultimate comfort. Even if he had then written, nothing was to affect their manners; what was next to be read. Lady Russell that Louisa Musgrove was to be introduced, and everybody was to tell her what no one so proper, so capable as Anne. I know he did not love her. Anne could not be sorry to be a great inclination to listen to. She disliked Bath, and did not want to be introduced.

As a comparison, here are paragraphs generated using the generic Markovify model.

- Such scarecrows as the only three of the navy. That young lady, the same brother must still be with them, he announced his intention of returning in the world. In Lady Russell's view, it was rather an early loss of her having no parent living, found a more equal match might have been enough, for he has not lost one charm of remembering former partialities and talking over old times. They came: the master of the house was undoubtedly superior to his sister. The rooms were shut up, the company assembled. The intervals of every sort to contend with, and in the world--nor the restrictions of the street; but his evident surprise and vexation at the Great House at rather an early breakfast hour, and as if they were got a much safer place for a time of life.
- I did not seem fit for the town, are what the commonest civility required. She therefore satisfied herself in the navy. My line of cliffs stretching out to the honour and received none: Elizabeth would, one day or other, be differently affected towards him. She was persuaded to think that he had probably been at all hours, of Mr Elliot had not, for many years, been on excellent terms, there being a sensible man. A few months ended Anne's share of the head, and sighs which declared his little faith in the back, as roused the most deplorable-looking personage you can yourself immediately contradict or confirm. That Louisa must remain a perfect impression of the circumstance of tolerable similarity, give such advice.

In generating many of these paragraphs, I noticed that the generic model consistently produced longer sentences (all of the paragraphs above contain 6 sentences each). I'm not sure why this is the case. 

My overall impression is that the paragraphs generated using the new approach seem to better imitate the rhythms of natural language and thus have a more "human" voice. To make it even more novel-like, it might be worth trying to model dialogue and narration separately, and then combine them based on the ratio of dialogue to narration in the real texts. It could also be fun purely to try to simulate a conversation in a Jane Austen novel. 